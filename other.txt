(decoder_blocks): ModuleList(
    (0-2): 3 x ResNetBlock(
      (act_fn): SiLU()
      (normalize1): GroupNorm(8, 512, eps=1e-05, affine=True)
      (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)
      (dense1): Linear(in_features=256, out_features=256, bias=True)
      (normalize2): GroupNorm(8, 256, eps=1e-05, affine=True)
      (dropout): Dropout2d(p=0.1, inplace=False)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)
      (match_input): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
      (attention): Identity()
    )
    (3): UpSample(
      (upsample): Sequential(
        (0): Upsample(scale_factor=2.0, mode='nearest')
        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
    (4-5): 2 x ResNetBlock(
      (act_fn): SiLU()
      (normalize1): GroupNorm(8, 512, eps=1e-05, affine=True)
      (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)
      (dense1): Linear(in_features=256, out_features=256, bias=True)
      (normalize2): GroupNorm(8, 256, eps=1e-05, affine=True)
      (dropout): Dropout2d(p=0.1, inplace=False)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)
      (match_input): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
      (attention): AttentionLayer(
        (group_norm): GroupNorm(8, 256, eps=1e-05, affine=True)
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
      )
    )
    (6): ResNetBlock(
      (act_fn): SiLU()
      (normalize1): GroupNorm(8, 384, eps=1e-05, affine=True)
      (conv1): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)
      (dense1): Linear(in_features=256, out_features=256, bias=True)
      (normalize2): GroupNorm(8, 256, eps=1e-05, affine=True)
      (dropout): Dropout2d(p=0.1, inplace=False)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)
      (match_input): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))
      (attention): AttentionLayer(
        (group_norm): GroupNorm(8, 256, eps=1e-05, affine=True)
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
      )
    )
    (7): UpSample(
      (upsample): Sequential(
        (0): Upsample(scale_factor=2.0, mode='nearest')
        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
    (8): ResNetBlock(
      (act_fn): SiLU()
      (normalize1): GroupNorm(8, 384, eps=1e-05, affine=True)
      (conv1): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)
      (dense1): Linear(in_features=256, out_features=128, bias=True)
      (normalize2): GroupNorm(8, 128, eps=1e-05, affine=True)
      (dropout): Dropout2d(p=0.1, inplace=False)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)
      (match_input): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1))
      (attention): AttentionLayer(
        (group_norm): GroupNorm(8, 128, eps=1e-05, affine=True)
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
        )
      )
    )
    (9): ResNetBlock(
      (act_fn): SiLU()
      (normalize1): GroupNorm(8, 256, eps=1e-05, affine=True)
      (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)
      (dense1): Linear(in_features=256, out_features=128, bias=True)
      (normalize2): GroupNorm(8, 128, eps=1e-05, affine=True)
      (dropout): Dropout2d(p=0.1, inplace=False)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)
      (match_input): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
      (attention): AttentionLayer(
        (group_norm): GroupNorm(8, 128, eps=1e-05, affine=True)
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
        )
      )
    )
    (10): ResNetBlock(
      (act_fn): SiLU()
      (normalize1): GroupNorm(8, 192, eps=1e-05, affine=True)
      (conv1): Conv2d(192, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)
      (dense1): Linear(in_features=256, out_features=128, bias=True)
      (normalize2): GroupNorm(8, 128, eps=1e-05, affine=True)
      (dropout): Dropout2d(p=0.1, inplace=False)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)
      (match_input): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1))
      (attention): AttentionLayer(
        (group_norm): GroupNorm(8, 128, eps=1e-05, affine=True)
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
        )
      )
    )
    (11): UpSample(
      (upsample): Sequential(
        (0): Upsample(scale_factor=2.0, mode='nearest')
        (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
    (12): ResNetBlock(
      (act_fn): SiLU()
      (normalize1): GroupNorm(8, 192, eps=1e-05, affine=True)
      (conv1): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)
      (dense1): Linear(in_features=256, out_features=64, bias=True)
      (normalize2): GroupNorm(8, 64, eps=1e-05, affine=True)
      (dropout): Dropout2d(p=0.1, inplace=False)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)
      (match_input): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))
      (attention): Identity()
    )
    (13-14): 2 x ResNetBlock(
      (act_fn): SiLU()
      (normalize1): GroupNorm(8, 128, eps=1e-05, affine=True)
      (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)
      (dense1): Linear(in_features=256, out_features=64, bias=True)
      (normalize2): GroupNorm(8, 64, eps=1e-05, affine=True)
      (dropout): Dropout2d(p=0.1, inplace=False)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)
      (match_input): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
      (attention): Identity()
    )
  )
  (final): Sequential(
    (0): GroupNorm(8, 64, eps=1e-05, affine=True)
    (1): SiLU()
    (2): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=same)
  )
)
